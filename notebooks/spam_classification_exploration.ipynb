{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcead4e",
   "metadata": {},
   "source": [
    "# Spam Email Classification - Interactive Exploration\n",
    "\n",
    "This notebook provides an interactive environment for exploring spam email classification using both traditional machine learning and deep learning approaches.\n",
    "\n",
    "## Features\n",
    "- Data exploration and visualization\n",
    "- Model training and evaluation\n",
    "- Interactive predictions\n",
    "- Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b44e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('src')\n",
    "\n",
    "# Import our custom modules\n",
    "from data_preprocessing import DataLoader, EmailPreprocessor\n",
    "from traditional_ml import TraditionalMLModels\n",
    "from deep_learning import DeepLearningModels, EnsemblePredictor\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac37cc5",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "loader = DataLoader()\n",
    "df = loader.load_spam_dataset()\n",
    "prepared_df = loader.prepare_dataset(df)\n",
    "\n",
    "print(f\"Dataset shape: {prepared_df.shape}\")\n",
    "print(f\"\\nColumn names: {list(prepared_df.columns)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(prepared_df['label'].value_counts())\n",
    "\n",
    "# Display first few rows\n",
    "prepared_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ca640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Pie chart\n",
    "label_counts = prepared_df['label'].value_counts()\n",
    "axes[0].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Email Label Distribution')\n",
    "\n",
    "# Bar chart\n",
    "label_counts.plot(kind='bar', ax=axes[1], color=['skyblue', 'lightcoral'])\n",
    "axes[1].set_title('Email Label Counts')\n",
    "axes[1].set_xlabel('Label')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Text length distribution\n",
    "axes[0, 0].hist(prepared_df['length'], bins=30, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Text Length Distribution')\n",
    "axes[0, 0].set_xlabel('Character Count')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "axes[0, 1].hist(prepared_df['num_words'], bins=30, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Word Count Distribution')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Text length by label\n",
    "spam_length = prepared_df[prepared_df['label'] == 'spam']['length']\n",
    "ham_length = prepared_df[prepared_df['label'] == 'ham']['length']\n",
    "\n",
    "axes[1, 0].hist([ham_length, spam_length], bins=20, alpha=0.7, \n",
    "                label=['Ham', 'Spam'], color=['skyblue', 'lightcoral'])\n",
    "axes[1, 0].set_title('Text Length by Label')\n",
    "axes[1, 0].set_xlabel('Character Count')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Box plot of features by label\n",
    "feature_cols = ['num_words', 'num_exclamation', 'num_uppercase', 'capital_ratio']\n",
    "melted_df = prepared_df[feature_cols + ['label']].melt(id_vars=['label'], var_name='feature', value_name='value')\n",
    "sns.boxplot(data=melted_df, x='feature', y='value', hue='label', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Feature Distribution by Label')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fddcfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word clouds for spam vs ham\n",
    "spam_text = ' '.join(prepared_df[prepared_df['label'] == 'spam']['cleaned_text'])\n",
    "ham_text = ' '.join(prepared_df[prepared_df['label'] == 'ham']['cleaned_text'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Spam word cloud\n",
    "if spam_text.strip():\n",
    "    spam_wordcloud = WordCloud(width=400, height=400, background_color='white').generate(spam_text)\n",
    "    axes[0].imshow(spam_wordcloud, interpolation='bilinear')\n",
    "    axes[0].set_title('Spam Email Word Cloud', fontsize=16)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "# Ham word cloud\n",
    "if ham_text.strip():\n",
    "    ham_wordcloud = WordCloud(width=400, height=400, background_color='white').generate(ham_text)\n",
    "    axes[1].imshow(ham_wordcloud, interpolation='bilinear')\n",
    "    axes[1].set_title('Ham Email Word Cloud', fontsize=16)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcec2e",
   "metadata": {},
   "source": [
    "## 2. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of numerical features\n",
    "feature_cols = ['length', 'num_words', 'num_sentences', 'num_exclamation', \n",
    "               'num_question', 'num_uppercase', 'num_digits', 'has_money_words', \n",
    "               'has_urgent_words', 'capital_ratio', 'target']\n",
    "\n",
    "correlation_matrix = prepared_df[feature_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c12af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for spam detection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare features\n",
    "X_features = prepared_df[feature_cols[:-1]]  # Exclude target\n",
    "y = prepared_df['target']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "# Train Random Forest for feature importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_scaled, y)\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols[:-1],\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.title('Feature Importance for Spam Detection')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367d7e9",
   "metadata": {},
   "source": [
    "## 3. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_text = prepared_df['text']\n",
    "X_cleaned = prepared_df['cleaned_text']\n",
    "y = prepared_df['target']\n",
    "\n",
    "X_train_text, X_test_text, X_train_cleaned, X_test_cleaned, y_train, y_test = train_test_split(\n",
    "    X_text, X_cleaned, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train_text)}\")\n",
    "print(f\"Test set size: {len(X_test_text)}\")\n",
    "print(f\"Class distribution in training set:\")\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac4312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train traditional ML models\n",
    "print(\"Training Traditional ML Models...\")\n",
    "ml_models = TraditionalMLModels()\n",
    "ml_models.train_all_models(X_train_cleaned, y_train, use_grid_search=False)\n",
    "\n",
    "# Evaluate ML models\n",
    "ml_results = ml_models.evaluate_all_models(X_test_cleaned, y_test)\n",
    "\n",
    "print(\"\\nML Model Results:\")\n",
    "for model_name, result in ml_results.items():\n",
    "    print(f\"{model_name}: F1={result['f1_score']:.4f}, AUC={result['auc_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ML model performance\n",
    "ml_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': model_name.replace('_', ' ').title(),\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'Precision': result['precision'],\n",
    "        'Recall': result['recall'],\n",
    "        'F1-Score': result['f1_score'],\n",
    "        'AUC': result['auc_score']\n",
    "    }\n",
    "    for model_name, result in ml_results.items()\n",
    "])\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Traditional ML Model Performance', fontsize=16)\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    bars = axes[row, col].bar(ml_comparison['Model'], ml_comparison[metric])\n",
    "    axes[row, col].set_title(metric)\n",
    "    axes[row, col].set_ylabel(metric)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, ml_comparison[metric]):\n",
    "        axes[row, col].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                          f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Remove the last subplot\n",
    "fig.delaxes(axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\nML Model Comparison:\")\n",
    "display(ml_comparison.sort_values('F1-Score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e46ab",
   "metadata": {},
   "source": [
    "## 4. Interactive Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa52fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction function\n",
    "def predict_email(text, show_individual=True):\n",
    "    \"\"\"Make predictions on email text.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EMAIL TEXT: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    ml_predictions = ml_models.predict(text)\n",
    "    \n",
    "    if show_individual:\n",
    "        print(\"\\nINDIVIDUAL MODEL PREDICTIONS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for model_name, pred in ml_predictions.items():\n",
    "            print(f\"{model_name.replace('_', ' ').title()}:\")\n",
    "            print(f\"  Prediction: {pred['prediction'].upper()}\")\n",
    "            print(f\"  Confidence: {pred['confidence']:.3f}\")\n",
    "            print(f\"  Spam Probability: {pred['spam_probability']:.3f}\")\n",
    "            print()\n",
    "    \n",
    "    # Calculate ensemble prediction\n",
    "    spam_probs = [pred['spam_probability'] for pred in ml_predictions.values()]\n",
    "    avg_spam_prob = np.mean(spam_probs)\n",
    "    ensemble_pred = 'spam' if avg_spam_prob > 0.5 else 'ham'\n",
    "    ensemble_conf = max(avg_spam_prob, 1 - avg_spam_prob)\n",
    "    \n",
    "    print(\"ENSEMBLE PREDICTION:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Prediction: {ensemble_pred.upper()}\")\n",
    "    print(f\"Confidence: {ensemble_conf:.3f}\")\n",
    "    print(f\"Spam Probability: {avg_spam_prob:.3f}\")\n",
    "    \n",
    "    return ensemble_pred, ensemble_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db123d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample spam email\n",
    "spam_sample = \"WINNER! You've won $1000! Click here to claim your prize NOW! Limited time offer!\"\n",
    "predict_email(spam_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf739f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample legitimate email\n",
    "ham_sample = \"Hey John, thanks for your help with the project yesterday. The presentation went really well!\"\n",
    "predict_email(ham_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692685ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction widget\n",
    "from IPython.widgets import interact, widgets\n",
    "\n",
    "# Sample texts for quick testing\n",
    "sample_texts = {\n",
    "    \"Spam 1\": \"URGENT! Your account will be suspended unless you verify immediately!\",\n",
    "    \"Spam 2\": \"FREE MONEY! No strings attached! Call now!!!\",\n",
    "    \"Ham 1\": \"Don't forget about mom's birthday next week.\",\n",
    "    \"Ham 2\": \"The weather is really nice today, perfect for a walk.\",\n",
    "    \"Custom\": \"\"\n",
    "}\n",
    "\n",
    "def interactive_predict(sample_choice, custom_text):\n",
    "    if sample_choice == \"Custom\":\n",
    "        text = custom_text\n",
    "    else:\n",
    "        text = sample_texts[sample_choice]\n",
    "    \n",
    "    if text.strip():\n",
    "        predict_email(text, show_individual=False)\n",
    "    else:\n",
    "        print(\"Please enter some text to classify.\")\n",
    "\n",
    "# Create interactive widget\n",
    "interact(\n",
    "    interactive_predict,\n",
    "    sample_choice=widgets.Dropdown(\n",
    "        options=list(sample_texts.keys()),\n",
    "        value=\"Spam 1\",\n",
    "        description=\"Sample:\"\n",
    "    ),\n",
    "    custom_text=widgets.Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter your custom email text here...\",\n",
    "        description=\"Custom Text:\",\n",
    "        layout=widgets.Layout(width='100%', height='100px')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce42f0",
   "metadata": {},
   "source": [
    "## 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model errors\n",
    "best_model_name = max(ml_results.keys(), key=lambda k: ml_results[k]['f1_score'])\n",
    "best_model = ml_models.trained_models[best_model_name]\n",
    "\n",
    "print(f\"Analyzing errors for best model: {best_model_name}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_model.predict(X_test_cleaned)\n",
    "y_pred_proba = best_model.predict_proba(X_test_cleaned)[:, 1]\n",
    "\n",
    "# Find misclassified examples\n",
    "misclassified_mask = y_pred != y_test\n",
    "false_positives = (y_pred == 1) & (y_test == 0)\n",
    "false_negatives = (y_pred == 0) & (y_test == 1)\n",
    "\n",
    "print(f\"Total misclassifications: {misclassified_mask.sum()}\")\n",
    "print(f\"False positives (ham â†’ spam): {false_positives.sum()}\")\n",
    "print(f\"False negatives (spam â†’ ham): {false_negatives.sum()}\")\n",
    "\n",
    "# Show some examples\n",
    "if false_positives.sum() > 0:\n",
    "    print(\"\\nFalse Positive Examples (Ham classified as Spam):\")\n",
    "    fp_indices = np.where(false_positives)[0][:3]\n",
    "    for i, idx in enumerate(fp_indices, 1):\n",
    "        print(f\"\\n{i}. Confidence: {y_pred_proba[idx]:.3f}\")\n",
    "        print(f\"   Text: {X_test_text.iloc[idx][:200]}...\")\n",
    "\n",
    "if false_negatives.sum() > 0:\n",
    "    print(\"\\nFalse Negative Examples (Spam classified as Ham):\")\n",
    "    fn_indices = np.where(false_negatives)[0][:3]\n",
    "    for i, idx in enumerate(fn_indices, 1):\n",
    "        print(f\"\\n{i}. Confidence: {1-y_pred_proba[idx]:.3f}\")\n",
    "        print(f\"   Text: {X_test_text.iloc[idx][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4763c6",
   "metadata": {},
   "source": [
    "## 6. Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955963fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from the best traditional model\n",
    "if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "    # For tree-based models\n",
    "    feature_names = best_model.named_steps['tfidf'].get_feature_names_out()\n",
    "    importances = best_model.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Get top features\n",
    "    top_indices = np.argsort(importances)[-20:]\n",
    "    top_features = [(feature_names[i], importances[i]) for i in top_indices]\n",
    "    \n",
    "    print(\"Top 20 Most Important Features:\")\n",
    "    for feature, importance in reversed(top_features):\n",
    "        print(f\"{feature:<20}: {importance:.4f}\")\n",
    "        \n",
    "    # Plot top features\n",
    "    features, values = zip(*top_features)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(len(features)), values)\n",
    "    plt.yticks(range(len(features)), features)\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top Features - {best_model_name.replace(\"_\", \" \").title()}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "elif hasattr(best_model.named_steps['classifier'], 'coef_'):\n",
    "    # For linear models\n",
    "    feature_names = best_model.named_steps['tfidf'].get_feature_names_out()\n",
    "    coefficients = best_model.named_steps['classifier'].coef_[0]\n",
    "    \n",
    "    # Get top positive and negative coefficients\n",
    "    top_positive_indices = np.argsort(coefficients)[-10:]\n",
    "    top_negative_indices = np.argsort(coefficients)[:10]\n",
    "    \n",
    "    print(\"Top 10 Spam Indicators (Positive Coefficients):\")\n",
    "    for i in reversed(top_positive_indices):\n",
    "        print(f\"{feature_names[i]:<20}: {coefficients[i]:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Ham Indicators (Negative Coefficients):\")\n",
    "    for i in top_negative_indices:\n",
    "        print(f\"{feature_names[i]:<20}: {coefficients[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037554f",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary\n",
    "print(\"SPAM CLASSIFICATION PROJECT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total samples: {len(prepared_df)}\")\n",
    "print(f\"  Training samples: {len(X_train_text)}\")\n",
    "print(f\"  Test samples: {len(X_test_text)}\")\n",
    "print(f\"  Spam percentage: {(prepared_df['target'].sum() / len(prepared_df)) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "best_result = ml_results[best_model_name]\n",
    "print(f\"  Best Model: {best_model_name.replace('_', ' ').title()}\")\n",
    "print(f\"  Accuracy: {best_result['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {best_result['precision']:.4f}\")\n",
    "print(f\"  Recall: {best_result['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {best_result['f1_score']:.4f}\")\n",
    "print(f\"  AUC Score: {best_result['auc_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"  â€¢ Traditional ML models are effective for spam classification\")\n",
    "print(f\"  â€¢ Text preprocessing significantly improves performance\")\n",
    "print(f\"  â€¢ Feature engineering (length, caps, punctuation) adds value\")\n",
    "print(f\"  â€¢ Ensemble methods can provide more robust predictions\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  â€¢ Try deep learning models (BERT, DistilBERT) for comparison\")\n",
    "print(f\"  â€¢ Collect more diverse training data\")\n",
    "print(f\"  â€¢ Implement real-time classification system\")\n",
    "print(f\"  â€¢ Add more sophisticated feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdb2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models for production use\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, f'models/{best_model_name}_model.joblib')\n",
    "print(f\"âœ“ Best model saved: models/{best_model_name}_model.joblib\")\n",
    "\n",
    "# Save preprocessing pipeline\n",
    "preprocessor = EmailPreprocessor()\n",
    "joblib.dump(preprocessor, 'models/email_preprocessor.joblib')\n",
    "print(\"âœ“ Preprocessor saved: models/email_preprocessor.joblib\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Spam classification project completed successfully!\")\n",
    "print(\"Models are ready for deployment in the web application.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
